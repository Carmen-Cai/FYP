{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAA-CNN_Model.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "_uBr9JVb7zWO",
        "bMcFYZ4y8Oq-",
        "c_YJqGSH8ZSj"
      ],
      "authorship_tag": "ABX9TyM6/BDI0l4BYZj8IW23Dyje",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Carmen-Cai/FYP/blob/main/GAA_CNN_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Settgings"
      ],
      "metadata": {
        "id": "B-xosFjC3bcq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Google docs"
      ],
      "metadata": {
        "id": "7rfrc10G4Hpw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')\n",
        "# !ls \"/content/drive/My Drive/FYP/ECGpdf2data/data/train/cropped_png/Recur\""
      ],
      "metadata": {
        "id": "PO1X1G6J4MZJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea38a2fe-aa27-4df4-a55e-deb31e6146bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##imports"
      ],
      "metadata": {
        "id": "9ocwwCYQ3uUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -U scikit-learn\n",
        "# !pip install visdom\n",
        "!pip install pytorchtools\n",
        "# from pytorchtools import EarlyStopping"
      ],
      "metadata": {
        "id": "8ynNWHFN34Xr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b956489a-4083-4bc0-de95-0e91df3543ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pytorchtools in /usr/local/lib/python3.7/dist-packages (0.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision.utils import save_image\n",
        "import torchvision.transforms as T \n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np \n",
        "import os\n",
        "from os import path\n",
        "#from google.colab import auth\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import pandas as pd\n",
        "import sklearn.metrics as metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from matplotlib import pyplot\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from statistics import mean\n",
        "import torch.utils.data as data\n",
        "# import EarlyStopping\n",
        "# from pytorchtools import EarlyStopping\n",
        "#auth.authenticate_user()\n",
        "\n",
        "#from visdom import Visdom \n",
        "\n",
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "-AHeF2Us3glh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a7419b3-ed17-4e48-cc96-844bf880787c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Parameter settings"
      ],
      "metadata": {
        "id": "h-yAjG_Y4RUZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args = {\n",
        "    #\"root\":\"/media/imed/GAA\",\n",
        "    \"root_GAA\":\"/content/drive/My Drive/FYP_Yixin_Cai/GAA_data\",#/origin_png/\",\n",
        "    \"root_CNN\":\"/content/drive/My Drive/FYP_Yixin_Cai/CNN_data\",\n",
        "    \"img_save_path\":\"./save_img\",\n",
        "    \"epoches\":20,#400,\n",
        "    \"lr\": 0.8e-4,#5, 来回 -》4\n",
        "    \"lr_cnn\":1e-3,\n",
        "    \"lr_ed\": 1e-3,\n",
        "    \"snapshot\":100,\n",
        "    \"test_step\":1,\n",
        "    \"ckpt_path\":\"/content/drive/My Drive/FYP_Yixin_Cai/GAA_data/checkpoint/\",\n",
        "    \"ckpt_path2\":\"/content/drive/My Drive/FYP_Yixin_Cai/CNN_data/checkpoint/\",\n",
        "    #\"ckpt_path\":\"/media/imed/Checkpoint/GAA/\",\n",
        "    \"batch_size\":25,\n",
        "    \"cnn_batch_size\":5,\n",
        "    \"name\":\"GAA\",\n",
        "    \"drop_out\":0.2,\n",
        "    \"best_acc\":0.9,\n",
        "    \"loss_img_folder\":3,\n",
        "    \"time\":0\n",
        "}"
      ],
      "metadata": {
        "id": "-RxMPgga4V2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Loading"
      ],
      "metadata": {
        "id": "Mu3LU1S65a-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Basic Functions"
      ],
      "metadata": {
        "id": "CYf1QTM25NEd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def random_HSV(image,h_limit=(-180,180),s_limit=(-255,255),v_limit=(-255,255),u=0.5):\n",
        "    if np.random.random() < u:\n",
        "        #rgb -> hsv\n",
        "        image = cv2.cvtColor(image,cv2.COLOR_BGR2HSV)\n",
        "        h,s,v = cv2.split(image)\n",
        "        h_shift = np.random.randint(h_limit[0],h_limit[1] + 1 )\n",
        "        h_shift = np.uint8(h_shift)\n",
        "        h += h_shift \n",
        "        s_shift = np.random.uniform(s_limit[0],s_limit[1])\n",
        "        s = cv2.add(s,s_shift)\n",
        "        v_shift = np.random.uniform(v_limit[0],v_limit[1])\n",
        "        v = cv2.add(v,v_shift)\n",
        "        image = cv2.merge((h,s,v))\n",
        "        image = cv2.cvtColor(image,cv2.COLOR_HSV2BGR)\n",
        "        # print(\"HSV image type\", type(image),image.shape)\n",
        "    return image\n",
        "\n",
        "def random_ShiftScaleRotate(image,mask,shift_limit=(-0.0,0.0),\n",
        "                            scale_limit=(-0.0,0.0),rotate_limit=(-0.0,0.0),\n",
        "                            aspect_limit=(-0.0,0.0),borderMode=cv2.BORDER_CONSTANT,u=0.5):\n",
        "\n",
        "    if np.random.random() < u:\n",
        "        height,width,channel = image.shape               \n",
        "        angle = np.random.uniform(rotate_limit[0],rotate_limit[1])\n",
        "        scale = np.random.uniform(1 + scale_limit[0],1 + scale_limit[1])\n",
        "        aspect = np.random.uniform(1 + aspect_limit[0],1 + aspect_limit[1])\n",
        "\n",
        "        sx = scale * aspect / (aspect ** 0.5)\n",
        "        sy = scale * (aspect ** 0.5)\n",
        "        dx = round(np.random.uniform(shift_limit[0],shift_limit[1]) * width)\n",
        "        dy = round(np.random.uniform(shift_limit[0],shift_limit[1]) * height)\n",
        "\n",
        "        cc = np.math.cos(angle / 180 * np.math.pi) * sx\n",
        "        ss = np.math.sin(angle / 180 * np.math.pi) * sy\n",
        "        rotate_matrix = np.array([[cc,-ss],[ss,cc]])\n",
        "\n",
        "        box0 = np.array([[0,0],[width,0],[width,height],[0,height]])\n",
        "        box1 = box0 - np.array([width / 2,height / 2])\n",
        "        box1 = np.dot(box1,rotate_matrix.T) + np.array([width / 2 + dx,height / 2 + dy])\n",
        "\n",
        "        box0 = box0.astype(np.float32)\n",
        "        box1 = box1.astype(np.float32)\n",
        "        mat = cv2.getPerspectiveTransform(box0,box1)\n",
        "        image = cv2.warpPerspective(image,mat,(width,height),flags=cv2.INTER_LINEAR,borderMode=borderMode,borderValue=(0,0,0))\n",
        "        mask = cv2.warpPerspective(mask,mat,(width,height),flags=cv2.INTER_LINEAR,borderMode=borderMode,borderValue=(0,0,0))\n",
        "\n",
        "    return image,mask\n",
        "\n",
        "\n",
        "def randomFlip_H(image,mask,u=0.5):\n",
        "    if np.random.random() < u:\n",
        "        image = cv2.flip(image, 1)\n",
        "        if type(mask)==type(np.array([1])):\n",
        "            mask = cv2.flip(mask,1)\n",
        "    return image,mask\n",
        "\n",
        "def randomFlip_V(image,mask,u=0.5):\n",
        "    if np.random.random() < u:\n",
        "        image = cv2.flip(image,0)\n",
        "        mask = cv2.flip(mask,0)\n",
        "    return image,mask\n",
        "\n",
        "def randomRotate_90(image,mask,u=0.5):\n",
        "    if np.random.random() < u:\n",
        "        image = np.rot90(image)\n",
        "        mask = np.rot90(mask)\n",
        "\n",
        "    return image,mask\n",
        "\n",
        "def get_one_hot(label,N):\n",
        "    size = list(label.size())\n",
        "    label = label.view(-1)\n",
        "    ones = torch.sparse.torch.eye(N)\n",
        "    ones = ones.index_select(0,label)\n",
        "    size.append(N)\n",
        "\n",
        "    return ones.view(*size)  \n",
        "\n",
        "def param_copy(param_s,param_t,alpha,epoch):\n",
        "    # param_s = list(param_s)\n",
        "    # param_t = list(param_t)\n",
        "    if epoch <= 1:\n",
        "        for ps,pt in zip(param_s,param_t):\n",
        "            pt.data[:] = ps.data[:]\n",
        "\n",
        "    else:\n",
        "        for ps,pt in zip(param_s,param_t):\n",
        "            pt.data.mul_(alpha)\n",
        "            pt.data.add_(ps.data * (1-alpha))"
      ],
      "metadata": {
        "id": "umF7AAiB5IZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.modules import loss\n",
        "\n",
        "def plot_loss(loss_dataset,loss_name,loss_dataset2=[],loss_name2=''):# visualize the loss as the network trained\n",
        "  \n",
        "  fig = plt.figure(figsize=(10,8))\n",
        "  plt.plot(range(1,len(loss_dataset)+1),loss_dataset, label=loss_name)\n",
        "  plt.ylim(0, 1.1*max(loss_dataset))\n",
        "  if loss_dataset2!=[]:\n",
        "    plt.plot(range(1,len(loss_dataset2)+1),loss_dataset2,label=loss_name2)\n",
        "    plt.ylim(0, 1.1*max(loss_dataset) if max(loss_dataset)> max(loss_dataset2) else 1.1*max(loss_dataset2))\n",
        "\n",
        "\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlim(0, len(loss_dataset)+1) # consistent scale\n",
        "  plt.grid(True)\n",
        "  plt.legend()\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "  img_name=loss_name+loss_name2 if loss_name2 != '' else loss_name\n",
        "  # fig.savefig('/content/drive/My Drive/FYP_Yixin_Cai/CNN_data/loss/loss_img/'+str(args['time'])+\"_\"+img_name+'.png', bbox_inches='tight' )"
      ],
      "metadata": {
        "id": "OZN_W_TS9BD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Class Image and Choroid_loader"
      ],
      "metadata": {
        "id": "_tIAYgfq6kZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageFloder(data.Dataset):\n",
        "    def __init__(self,root_path,datasets=\"GAA\",mode=\"train\"):\n",
        "        self.root = root_path\n",
        "        self.mode = mode\n",
        "        self.dataset = datasets\n",
        "\n",
        "        if self.dataset == \"GAA\":#\"CHOROID\":\n",
        "            if self.mode==\"train\":\n",
        "                # self.images, self.labels, self.img_target, self.mask_target = read_datasets(self.root,self.mode) #mask_target is the label for cnn model\n",
        "                self.images, self.labels, self.img_target = read_datasets(self.root,self.mode,self.dataset)\n",
        "            \n",
        "            if self.mode==\"validation\":\n",
        "                # self.images, self.labels, self.img_target, self.mask_target = read_datasets(self.root,self.mode) #mask_target is the label for cnn model\n",
        "                self.images, self.labels, self.img_target = read_datasets(self.root,self.mode)\n",
        "            \n",
        "            if self.mode == \"test\":\n",
        "                self.images, self.labels = read_datasets(self.root, self.mode) #labels is the label for cnn model\n",
        "        \n",
        "        if self.dataset == \"CNN\":\n",
        "            self.images, self.labels = read_datasets(self.root,self.mode,self.dataset)\n",
        "            \n",
        "    \n",
        "    def __getitem__(self,index):\n",
        "      if self.dataset == \"GAA\":#\"CHOROID\":\n",
        "        if self.mode == \"train\" :#or self.mode == \"test\":\n",
        "            img, mask, img_t = default_loader(mode=self.mode,img_path=self.images[index], mask_path=self.labels[index],\n",
        "                                      img_t_path=self.img_target[index])#,mask_t_path=self.mask_target[index])\n",
        "            # img, mask, img_t, mask_t = default_loader(mode=self.mode,img_path=self.images[index], mask_path=self.labels[index],\n",
        "            # img_t_path=self.img_target[index])#,mask_t_path=self.mask_target[index])\n",
        "            img = torch.Tensor(img)\n",
        "            mask = torch.Tensor(mask)\n",
        "            img_t = torch.Tensor(img_t)\n",
        "            # print(\"default loader, the size of pdf images are: \",img.size())\n",
        "            # print(\"default loader, the size of target images are: \",img_t.size())\n",
        "            return img,mask,img_t#,mask_t#,self.target_name[index]\n",
        "\n",
        "        if self.mode == \"validation\" :#or self.mode == \"test\":\n",
        "            img, mask, img_t = default_loader(mode=self.mode,img_path=self.images[index], mask_path=self.labels[index],\n",
        "                                      img_t_path=self.img_target[index])#,mask_t_path=self.mask_target[index])\n",
        "            # img, mask, img_t, mask_t = default_loader(mode=self.mode,img_path=self.images[index], mask_path=self.labels[index],\n",
        "            # img_t_path=self.img_target[index])#,mask_t_path=self.mask_target[index])\n",
        "            img = torch.Tensor(img)\n",
        "            mask = torch.Tensor(mask)\n",
        "            img_t = torch.Tensor(img_t)\n",
        "            return img,mask,img_t#,mask_t#,self.target_name[index]\n",
        "\n",
        "        if self.mode == \"test\" :\n",
        "            img= default_loader(mode=self.mode,img_path=self.images[index])\n",
        "            # img, mask= default_loader(mode=self.mode,img_path=self.images[index], mask_path=self.labels[index])\n",
        "            img = torch.Tensor(img)\n",
        "            return img,self.labels[index] #,self.target_name[index]\n",
        "\n",
        "\n",
        "      if self.dataset == \"CNN\":\n",
        "        img= default_loader(mode=self.mode,img_path=self.images[index])\n",
        "        img = torch.Tensor(img)\n",
        "        return img, self.labels[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        assert len(self.images) == len(self.labels)\n",
        "        return len(self.images)\n",
        "\n",
        "class Choroid_loader:\n",
        "    def __init__(self,root_path,datasets):\n",
        "        self.root_path = root_path\n",
        "        self.datasets = datasets\n",
        "        pass\n",
        "    def load_train_data(self,batch_size):\n",
        "        dataset = ImageFloder(self.root_path,self.datasets,mode=\"train\")\n",
        "        train_loader = data.DataLoader(dataset,batch_size,num_workers=0,shuffle=True,pin_memory=True)\n",
        "        return train_loader\n",
        "\n",
        "    def load_val_data(self,batch_size):\n",
        "        dataset = ImageFloder(self.root_path,self.datasets,mode=\"validation\")\n",
        "        test_loader = data.DataLoader(dataset,batch_size,num_workers=0,shuffle=True,pin_memory=True)\n",
        "        return test_loader\n",
        "\n",
        "    def load_test_data(self,batch_size):\n",
        "        dataset = ImageFloder(self.root_path,self.datasets,mode=\"test\")\n",
        "        test_loader = data.DataLoader(dataset,batch_size,num_workers=0,shuffle=True,pin_memory=True)\n",
        "        return test_loader\n",
        "\n",
        "    # def load_pred_data(self):\n",
        "    #     dataset = ImageFloder(self.root_path,self.datasets,mode=\"pred\")\n",
        "    #     pred_loader = data.DataLoader(dataset,batch_size=1,num_workers=0,shuffle=False,pin_memory=True)\n",
        "    #     return pred_loader,dataset.images\n"
      ],
      "metadata": {
        "id": "CyhpolyL6-zL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Read_dataset"
      ],
      "metadata": {
        "id": "BXSpUd0M6lFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_datasets(root_path,mode=\"train\",dataset=\"GAA\"):\n",
        "\n",
        "    images = []\n",
        "    masks = []\n",
        "    images_target=[]\n",
        "    images_name=[]\n",
        "    labels=[]\n",
        "    #mask_target=[]\n",
        "    if dataset==\"GAA\":\n",
        "      if mode == \"train\":\n",
        "          outer_path = os.path.join(root_path,\"train\")\n",
        "          image_path = os.path.join(outer_path, \"cropped_png/cropped\") \n",
        "          label_path = os.path.join(outer_path, \"extracted_waveforms/extracted\")\n",
        "          image_target_path = os.path.join(outer_path, \"digitized_paper\")\n",
        "          #mask_target_path = os.path.join(outer_path, \"digitized_paper_diagnosis.csv\")\n",
        "\n",
        "          image_list = os.listdir(image_path)\n",
        "          image_list = [x for x in image_list if '.DS_Store' not in x]\n",
        "          image_list = [x for x in image_list if '.ipynb_checkpoints' not in x]\n",
        "\n",
        "          image_target_list = os.listdir(image_target_path)\n",
        "          image_target_list = [x for x in image_target_list if '.DS_Store' not in x]\n",
        "          image_target_list = [x for x in image_target_list if '.ipynb_checkpoints' not in x]\n",
        "\n",
        "          #mask_target_df = pd.read_csv(mask_target_path)\n",
        "\n",
        "          for image in image_list:\n",
        "              imagePath = os.path.join(image_path, image)\n",
        "              images.append(imagePath) #867\n",
        "\n",
        "              maskPath = os.path.join(label_path, image)\n",
        "              masks.append(maskPath) #867\n",
        "          # print(len(images),len(masks))\n",
        "          for image in image_target_list:\n",
        "              imagePath = os.path.join(image_target_path, image)\n",
        "              images_target.append(imagePath) #867\n",
        "              #result = mask_target_df[mask_target_df['ID'] == int(image[0:-4])]\n",
        "              #label = result['Diagnosis'].values[0]  # 应该加在default loader里面\n",
        "              #mask_target.append(label)\n",
        "          # print(len(images_target))\n",
        "          return images, masks, images_target #, mask_target #lists of addresses\n",
        "      \n",
        "      if mode == \"validation\":\n",
        "          outer_path = os.path.join(root_path,\"validation\")\n",
        "          image_path = os.path.join(outer_path, \"cropped_png/\") \n",
        "          label_path = os.path.join(outer_path, \"extracted_waveforms/\")\n",
        "          image_target_path = os.path.join(outer_path, \"digitized_paper\")\n",
        "          #mask_target_path = os.path.join(outer_path, \"digitized_paper_diagnosis.csv\")\n",
        "\n",
        "          image_list = os.listdir(image_path)\n",
        "          image_list = [x for x in image_list if '.DS_Store' not in x]\n",
        "          image_list = [x for x in image_list if '.ipynb_checkpoints' not in x]\n",
        "\n",
        "          image_target_list = os.listdir(image_target_path)\n",
        "          image_target_list = [x for x in image_target_list if '.DS_Store' not in x]\n",
        "          image_target_list = [x for x in image_target_list if '.ipynb_checkpoints' not in x]\n",
        "\n",
        "          #mask_target_df = pd.read_csv(mask_target_path)\n",
        "\n",
        "          for image in image_list:\n",
        "              imagePath = os.path.join(image_path, image)\n",
        "              images.append(imagePath) #108\n",
        "\n",
        "              maskPath = os.path.join(label_path, image)\n",
        "              masks.append(maskPath) #108\n",
        "          # print(len(images),len(masks))\n",
        "          for image in image_target_list:\n",
        "              imagePath = os.path.join(image_target_path, image)\n",
        "              images_target.append(imagePath) #108\n",
        "              #result = mask_target_df[mask_target_df['ID'] == int(image[0:-4])]\n",
        "              #label = result['Diagnosis'].values[0]  # 应该加在default loader里面\n",
        "              #mask_target.append(label)\n",
        "          # print(len(images_target))\n",
        "          return images, masks, images_target #, mask_target #lists of addresses\n",
        "\n",
        "      if mode == \"test\":\n",
        "          outer_path = os.path.join(root_path,\"test\")\n",
        "          image_target_path = os.path.join(outer_path, \"digitized_paper\")\n",
        "          # mask_target_path = os.path.join(outer_path, \"mask/digitized_paper_diagnosis.csv\")\n",
        "\n",
        "          image_target_list = os.listdir(image_target_path)\n",
        "          image_target_list = [x for x in image_target_list if '.DS_Store' not in x]\n",
        "          image_target_list = [x for x in image_target_list if '.ipynb_checkpoints' not in x]\n",
        "          \n",
        "          # mask_target_df=pd.read_csv(mask_target_path)\n",
        "\n",
        "          for image in image_target_list:\n",
        "              imagePath = os.path.join(image_target_path, image)\n",
        "              images_target.append(imagePath)\n",
        "              images_name.append(image[0:-4])\n",
        "\n",
        "              # result = mask_target_df[mask_target_df['ID'] == int(image[0:-4])]\n",
        "              # label = result['Diagnosis'].values[0]  # 应该加在default loader里面\n",
        "              # mask_target.append(label)\n",
        "\n",
        "          return images_target,images_name  #, mask_target\n",
        "\n",
        "\n",
        "    if dataset==\"CNN\":\n",
        "      # if mode == \"train\":\n",
        "      outer_path = os.path.join(root_path,mode)\n",
        "      image_path = os.path.join(outer_path, \"extracted_digitized_paper/binary_image_1\") \n",
        "      label_path = os.path.join(root_path, \"digitized_paper_diagnosis.csv\")\n",
        "      # image_target_path = os.path.join(outer_path, \"digitized_paper\")\n",
        "      image_list = os.listdir(image_path)\n",
        "      image_list = [x for x in image_list if '.DS_Store' not in x]\n",
        "      image_list = [x for x in image_list if '.ipynb_checkpoints' not in x]\n",
        "\n",
        "      label_df = pd.read_csv(label_path)\n",
        "\n",
        "      for image in image_list:\n",
        "          imagePath = os.path.join(image_path, image)\n",
        "          images.append(imagePath) #138\n",
        "          \n",
        "          result = label_df[label_df['ID'] == int(image[5:-5])]\n",
        "          label = result['Diagnosis'].values[0]  # 应该加在default loader里面\n",
        "          labels.append(label)\n",
        "      # print(\"label len:\", len(labels))\n",
        "      # print(\"image len\",len(images))\n",
        "\n",
        "      return images, labels\n"
      ],
      "metadata": {
        "id": "RNGfhPji6jGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Default Loader"
      ],
      "metadata": {
        "id": "G8Xg6sf559vd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def default_loader(mode,img_path,mask_path=\"blank\",img_t_path=\"blank\",mask_t_path=\"blank\"):\n",
        "    # dim0=(512,512)\n",
        "    dim=(128,128)\n",
        "    # print(img_path,mask_path,img_t_path,mask_t_path)\n",
        "\n",
        "    if mode==\"test\" or mask_path==\"blank\":\n",
        "      img= random_crop(img_path,mask_path,dim,\"test\")\n",
        "      img = random_HSV(img, h_limit=(-30, 30), s_limit=(-5, 5), v_limit=(-15, 15))\n",
        "      img = np.array(img, dtype=np.float32).transpose(2, 0, 1) / 255  # * 3.2 - 1.6\n",
        "      return img\n",
        "\n",
        "    img,mask = random_crop(img_path,mask_path,dim,\"otherMode\")\n",
        "    img = random_HSV(img, h_limit=(-30, 30), s_limit=(-5, 5), v_limit=(-15, 15))\n",
        "    img, mask = randomFlip_H(img, mask)\n",
        "    img = np.array(img, dtype=np.float32).transpose(2, 0, 1) / 255  # * 3.2 - 1.6\n",
        "    \n",
        "    # if mode == \"train\" :\n",
        "    img_target = random_crop(img_t_path, mask_t_path, dim,\"otherMode\")\n",
        "\n",
        "    img_target = random_HSV(img_target,h_limit=(-30,30),s_limit=(-5,5),v_limit=(-15,15))\n",
        "\n",
        "    # img_target,mask_target= randomFlip_H(img_target,mask_target)\n",
        "\n",
        "    if type(mask)==type(np.array([1])):\n",
        "        mask = mask[:, :].copy()\n",
        "        mask = torch.LongTensor(mask)\n",
        "\n",
        "        mask1=mask.detach()\n",
        "        mask1[mask1<112]=0\n",
        "        mask1[mask1>=112]=255\n",
        "        mask1 = mask1.reshape(mask.shape[0], mask.shape[1], 1)\n",
        "\n",
        "        mask1 = np.array(mask1, dtype=np.float32).transpose(2, 0, 1)/255\n",
        "\n",
        "    # if type(mask_target)==type(np.array([1])):\n",
        "    #     mask_target = mask_target[:, :].copy()\n",
        "    #     mask_target = torch.LongTensor(mask_target)\n",
        "    #     mask_target = mask_target.numpy()\n",
        "    #     mask_target = mask_target.reshape(mask_target.shape[0], mask_target.shape[1], 1)\n",
        "    #     mask_target = np.array(mask_target, dtype=np.float32).transpose(2, 0, 1)/255\n",
        "\n",
        "    img_target = np.array(img_target, dtype=np.float32).transpose(2, 0, 1) / 255  # * 3.2 - 1.6\n",
        "\n",
        "    return img, mask1, img_target#, mask_target"
      ],
      "metadata": {
        "id": "MppPWmuF6Q9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Random Crop"
      ],
      "metadata": {
        "id": "CunFTYHe5dtU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def random_crop(img_path=\"blank\",mask_path=\"blank\",dim=(0,0,0),mode=\"blank\"):\n",
        "\n",
        "  img = cv2.imread(img_path)\n",
        "  resized_img = cv2.resize(img, dim, interpolation=cv2.INTER_AREA)#(512, 512, 3)\n",
        "  resized_img = np.asarray(resized_img)\n",
        "\n",
        "  if mode ==\"test\":\n",
        "    # cv2_imshow(img)\n",
        "    # print(mask_path)\n",
        "    return resized_img\n",
        "\n",
        "  if mask_path==\"blank\": #mask is label for cnn model in train and test mode\\\n",
        "\n",
        "      return resized_img#, mask_path\n",
        "  else:\n",
        "    mask = cv2.imread(mask_path,cv2.IMREAD_GRAYSCALE)\n",
        "    # print(mask.shape)\n",
        "    resized_mask = cv2.resize(mask, dim, interpolation=cv2.INTER_NEAREST)  # (512, 512, 3)\n",
        "\n",
        "    resized_mask = np.asarray(resized_mask)\n",
        "    resized_mask = resized_mask.reshape(resized_mask.shape[0], resized_mask.shape[1], 1)\n",
        "\n",
        "    \n",
        "    return resized_img,resized_mask"
      ],
      "metadata": {
        "id": "ATlWBtFV5cWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Models"
      ],
      "metadata": {
        "id": "1Rbw06OD7fYd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##CNN"
      ],
      "metadata": {
        "id": "i8eK-CrU7iwC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self,num_classes=1):\n",
        "        super().__init__()\n",
        "        # print(\"**********CNN********** \")\n",
        "        self.features=nn.Sequential(###512 to 1\n",
        "        ##half\n",
        "        nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
        "        nn.BatchNorm2d(32, eps=0.001),\n",
        "        nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "        nn.BatchNorm2d(64, eps=0.001),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "        #half\n",
        "        nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "        nn.BatchNorm2d(128, eps=0.001),\n",
        "        nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
        "        nn.BatchNorm2d(128, eps=0.001),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "        ##half\n",
        "        nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "        nn.BatchNorm2d(256, eps=0.001),\n",
        "        nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "        nn.BatchNorm2d(256, eps=0.001),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "        )\n",
        "\n",
        "        self.classifier=nn.Sequential(\n",
        "            nn.Linear(16 * 16 * 256, 2048),\n",
        "            nn.BatchNorm1d(2048, eps=0.001),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(2048,num_classes),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        x=self.features(x)#torch.Size([2, 256, 16, 16])\n",
        "        x = x.view(x.size(0), 16 * 16 * 256)\n",
        "        x=self.classifier(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "CoQz7CgH7gbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Metrics"
      ],
      "metadata": {
        "id": "_uBr9JVb7zWO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch._C import set_num_threads\n",
        "def equal(num1,num2,type=1):\n",
        "  if type==0: ##check if not equal\n",
        "    equal=(num1!=num2)\n",
        "    equal=np.array(equal).astype('int32')\n",
        "    return equal\n",
        "\n",
        "  ##check if equal\n",
        "  equal=(num1==num2)\n",
        "  equal=np.array(equal).astype('int32')\n",
        "  return equal\n",
        "\n",
        "\n",
        "def numeric_score_original(pred,gt,j):\n",
        "    FP = np.sum(equal(equal(pred,j),equal(gt,j,type=0)))#np.float(np.sum(equal(equal(pred,j),equal(gt,j,type=0))))\n",
        "    FN = np.sum(equal(equal(pred,j,type=0),equal(gt,j)))#np.float(np.sum(equal(equal(pred,j,type=0),equal(gt,j))))\n",
        "    TP = np.sum(equal(equal(pred,j),equal(gt,j)))#np.float(np.sum(equal(equal(pred,j),equal(gt,j))))\n",
        "    TN = np.sum(equal(equal(pred,j,type=0),equal(gt,j,type=0)))#np.float(np.sum(equal(equal(pred,j,type=0),equal(gt,j,type=0))))\n",
        "\n",
        "    return FP,FN,TP,TN\n",
        "\n",
        "def numeric_score(pred,gt,classes):\n",
        "    FP_list = []\n",
        "    FN_list = []\n",
        "    TP_list = []\n",
        "    TN_list = []\n",
        "    for i in range(1,classes):\n",
        "        FPI,FNI,TPI,TNI = numeric_score_original(pred,gt,i)\n",
        "        FP_list.append(FPI)\n",
        "        FN_list.append(FNI)\n",
        "        TP_list.append(TPI)\n",
        "        TN_list.append(TNI)\n",
        "\n",
        "    FP = np.mean(FP_list)\n",
        "    FN = np.mean(FN_list)\n",
        "    TP = np.mean(TP_list)\n",
        "    TN = np.mean(TN_list)\n",
        "\n",
        "    return FP,FN,TP,TN\n",
        "\n",
        "def get_acc(image,label):\n",
        "\n",
        "    FP,FN,TP,TN = numeric_score(image,label)\n",
        "    acc = (TP + TN) / (TP + FN + TN + FP + 1e-10)\n",
        "    sen = (TP) / (TP + FN + 1e-10)\n",
        "    return acc,sen\n",
        "\n",
        "def metrics_self(pred,label,batch_size):\n",
        "    outputs = (pred.data.cpu().numpy()).astype(np.uint8)\n",
        "    label = (label.data.cpu().numpy()).astype(np.uint8)\n",
        "\n",
        "    Acc,Sen = 0. ,0.\n",
        "    for i in range(batch_size):\n",
        "        img = outputs[i,:,:,:]\n",
        "        gt = label[i,:,:,:]\n",
        "\n",
        "        acc,sen = get_acc(img,gt)\n",
        "        Acc += acc\n",
        "        Sen += sen\n",
        "\n",
        "    return Acc,Sen\n",
        "\n",
        "def Auc_score(SR,GT,threshold=0.5):\n",
        "    GT = GT.ravel()\n",
        "    SR = SR.ravel()\n",
        "\n",
        "    roc_auc = metrics.roc_auc_score(GT,SR,sample_weight=0.5)\n",
        "\n",
        "    return roc_auc\n",
        "\n",
        "def all_score(prediction,groundtruth,classes):\n",
        "    FP,FN,TP,TN = numeric_score(prediction,groundtruth,classes)\n",
        "\n",
        "    acc = (TP + TN) / (TP + FN + TN + FP + 1e-10)\n",
        "\n",
        "    sen = (TP) / (TP + FN + 1e-10)\n",
        "\n",
        "    pre = TP / (TP + FP + 1e-10)\n",
        "\n",
        "    f1 = (2 * pre * sen) / (pre + sen + 1e-10)\n",
        "\n",
        "    outputs = prediction > 0.5\n",
        "    masks = groundtruth == np.max(groundtruth)\n",
        "    inter = np.sum(outputs * masks)\n",
        "    dc = 2 * inter\n",
        "\n",
        "    return acc,sen,pre,f1#,auc"
      ],
      "metadata": {
        "id": "-5tzQ7u_736a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Unet"
      ],
      "metadata": {
        "id": "irBOq8NT8GJK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels):\n",
        "        super().__init__()\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels,out_channels,kernel_size=3,padding=1,bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels,out_channels,kernel_size=3,padding=1,bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "    def forward(self,x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "class Down(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels):\n",
        "        super().__init__()\n",
        "        self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(in_channels,out_channels)\n",
        "        )\n",
        "    def forward(self,x):    \n",
        "        return self.maxpool_conv(x)\n",
        "\n",
        "class Up(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels,bilinear=True):\n",
        "        super().__init__()\n",
        "        if bilinear:\n",
        "            self.up = nn.Upsample(scale_factor=2,mode=\"bilinear\",align_corners=True)\n",
        "        else:\n",
        "            self.up = nn.ConvTranspose2d(in_channels // 2,in_channels // 2,kernel_size=2,stride=2)\n",
        "        self.conv = DoubleConv(in_channels,out_channels)\n",
        "    def forward(self,x1,x2):\n",
        "        x1 = self.up(x1)\n",
        "        x = torch.cat([x2,x1],dim = 1)\n",
        "        return self.conv(x)\n",
        "\n",
        "class OutConv(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_channels,out_channels,1)\n",
        "    def forward(self,x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class Unet(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels,bilinear=True):\n",
        "        super().__init__()\n",
        "        self.biliner = bilinear\n",
        "        self.inchannel = DoubleConv(in_channels,64)\n",
        "        self.down1 = Down(64,128)\n",
        "        self.down2 = Down(128,256)\n",
        "        self.down3 = Down(256,512)\n",
        "        self.down4 = Down(512,512)\n",
        "        self.up1 = Up(1024,256,bilinear)\n",
        "        self.up2 = Up(512,128,bilinear)\n",
        "        self.up3 = Up(256,64,bilinear)\n",
        "        self.up4 = Up(128,64,bilinear)\n",
        "        self.outchannel = OutConv(64,out_channels)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self,x):\n",
        "        x1 = self.inchannel(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        x = self.up1(x5,x4)\n",
        "        x = self.up2(x,x3)\n",
        "        x = self.up3(x,x2)\n",
        "        x = self.up4(x,x1)\n",
        "        out = self.outchannel(x)\n",
        "        out = self.sigmoid(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self,in_channels):\n",
        "        super().__init__()\n",
        "        self.inchannel = DoubleConv(in_channels,64)\n",
        "        self.down1 = Down(64,128)\n",
        "        self.down2 = Down(128,256)\n",
        "        self.down3 = Down(256,512)\n",
        "        self.down4 = Down(512,512)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x1 = self.inchannel(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "\n",
        "        return x1,x2,x3,x4,x5\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self,out_channels,bilinear=True):\n",
        "        super().__init__()\n",
        "        self.up1 = Up(1024,256,bilinear)\n",
        "        self.up2 = Up(512,128,bilinear)\n",
        "        self.up3 = Up(256,64,bilinear)\n",
        "        self.up4 = Up(128,64,bilinear)\n",
        "        self.outchannel = OutConv(64,out_channels)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        #self.cnn=\n",
        "\n",
        "    def forward(self,x1,x2,x3,x4,x5):\n",
        "        x = self.up1(x5,x4)\n",
        "        x = self.up2(x,x3)\n",
        "        x = self.up3(x,x2)\n",
        "        x = self.up4(x,x1)\n",
        "        out = self.outchannel(x)\n",
        "        out = self.sigmoid(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "HS22U2Qj8GzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##E Discriminator"
      ],
      "metadata": {
        "id": "bMcFYZ4y8Oq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder_Discriminator(nn.Module):\n",
        "    def __init__(self,size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(512,256,3,1,1)\n",
        "        self.bn1 = nn.BatchNorm2d(256)\n",
        "        self.conv2 = nn.Conv2d(256,128,3,1,1)\n",
        "        self.bn2 = nn.BatchNorm2d(128)\n",
        "        self.conv3 = nn.Conv2d(128,64,3,1,1)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "        self.conv4 = nn.Conv2d(64,32,3,1,1)\n",
        "        self.bn4 = nn.BatchNorm2d(32)\n",
        "\n",
        "        self.leakyrelu1 = nn.LeakyReLU(0.2,inplace=True)\n",
        "        self.leakyrelu2 = nn.LeakyReLU(0.2,inplace=True)\n",
        "        self.leakyrelu3 = nn.LeakyReLU(0.2,inplace=True)\n",
        "        self.leakyrelu4 = nn.LeakyReLU(0.2,inplace=True)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear = nn.Linear(8*8*size,1)#128? =4*32768/32/32\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.leakyrelu1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.leakyrelu2(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x)\n",
        "        x = self.leakyrelu3(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.bn4(x)\n",
        "        x = self.leakyrelu4(x)\n",
        "\n",
        "        x = self.flatten(x)\n",
        "        x = self.linear(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.sigmoid(x)\n",
        "\n",
        "        return x "
      ],
      "metadata": {
        "id": "8v773gBg8RdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Discriminator_res"
      ],
      "metadata": {
        "id": "c_YJqGSH8ZSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Res_Block(nn.Module):\n",
        "    def __init__(self,inchannal,outchannal,s=1,shortcut=None):\n",
        "        super().__init__()\n",
        "        self.left = nn.Sequential(\n",
        "            nn.Conv2d(inchannal,outchannal,3,s,1),\n",
        "            nn.BatchNorm2d(outchannal),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(outchannal,outchannal,3,1,1),\n",
        "            nn.BatchNorm2d(outchannal)\n",
        "        )\n",
        "\n",
        "        self.right = shortcut\n",
        "\n",
        "    def forward(self,x):\n",
        "        out = self.left(x)\n",
        "        res = x if self.right == None else self.right(x)\n",
        "        out += res\n",
        "        out = nn.functional.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class Resnet34(nn.Module):\n",
        "    def __init__(self,classes):\n",
        "        super().__init__()\n",
        "        self.first = nn.Sequential(\n",
        "            nn.Conv2d(classes,64,7,2,3),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(3,2,1)\n",
        "        )\n",
        "        \n",
        "        self.res1 = self.Block_Layer(64,64,3)\n",
        "        self.res2 = self.Block_Layer(64,128,4,s=2)\n",
        "        self.res3 = self.Block_Layer(128,256,6,s=2)\n",
        "        self.res4 = self.Block_Layer(256,512,3,s=2)\n",
        "\n",
        "        self.conv11 = nn.Conv2d(512,1,1,1,0)\n",
        "        self.global_pool = nn.AdaptiveAvgPool2d((1,1))\n",
        "        self.linear = nn.Linear(512,1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def Block_Layer(self,inchannel,outchannel,num,s=1):\n",
        "        shortcut = nn.Sequential(\n",
        "            nn.Conv2d(inchannel,outchannel,3,s,1,bias=False),\n",
        "            nn.BatchNorm2d(outchannel)\n",
        "        )\n",
        "\n",
        "        layer = []\n",
        "        layer.append(Res_Block(inchannel,outchannel,s,shortcut)) \n",
        "        for i in range(1,num):\n",
        "            layer.append(Res_Block(outchannel,outchannel))\n",
        "\n",
        "        return nn.Sequential(*layer)\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "        x = self.first(x)\n",
        "        x = self.res1(x)\n",
        "        x = self.res2(x)\n",
        "        x = self.res3(x)\n",
        "        x = self.res4(x)\n",
        "        x = self.conv11(x)\n",
        "        x = self.sigmoid(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "zT3_WR9W8cQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train\n"
      ],
      "metadata": {
        "id": "bakMdn8b8lxy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**GAA_Train**"
      ],
      "metadata": {
        "id": "odqQ5Ogu9LxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import random_split\n",
        "class Trainers:\n",
        "    def __init__(self):\n",
        "        data_loader = Choroid_loader(root_path=args[\"root_GAA\"],datasets=\"GAA\")\n",
        "        self.train_loader = data_loader.load_train_data(batch_size=args[\"batch_size\"])\n",
        "        self.val_loader = data_loader.load_val_data(batch_size=args[\"batch_size\"])\n",
        "        self.test_loader = data_loader.load_test_data(batch_size=args['batch_size'])\n",
        "\n",
        "    def save_ckpt(self,encoder_s,encoder_t,decoder,epoch):\n",
        "        if not os.path.exists(args[\"ckpt_path\"]):\n",
        "            os.makedirs(args[\"ckpt_path\"])\n",
        "\n",
        "        state = {\"encoder_s\":encoder_s.state_dict(),\"encoder_t\":encoder_t.state_dict(),\"decoder\":decoder.state_dict()}\n",
        "        torch.save(state,args[\"ckpt_path\"] + args[\"name\"] + \"_epoch_\" + str(epoch)+ \".pkl\")\n",
        "        print(\"---> save model:{} <---\".format(args[\"ckpt_path\"]))\n",
        "        print(args[\"ckpt_path\"] + args[\"name\"] + \"_epoch_\" + str(epoch) + \".pkl\")\n",
        "\n",
        "    def get_ckpt(self,epoch):\n",
        "        encoder_s = Encoder(3).cuda() #source encoder\n",
        "        encoder_t = Encoder(3).cuda() #target encoder\n",
        "        decoder = Decoder(1).cuda() #6\n",
        "\n",
        "        checkpoint = torch.load(args[\"ckpt_path\"] + args[\"name\"] + \"_epoch_\" + str(epoch) + \".pkl\")\n",
        "        encoder_s.load_state_dict(checkpoint['encoder_s'])\n",
        "        encoder_t.load_state_dict(checkpoint['encoder_t'])\n",
        "        decoder.load_state_dict(checkpoint['decoder'])\n",
        "\n",
        "        return encoder_s,encoder_t,decoder\n",
        "\n",
        "    def adjust_lr(self,optimizer,base_lr,iter,max_iter,power=0.9):\n",
        "        lr = base_lr * (1 - float(iter) / max_iter) ** power\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group[\"lr\"] = lr \n",
        "\n",
        "    def model_test(self,epoch):\n",
        "      print(\"----------Start testing model----------\")\n",
        "      accuracy=0\n",
        "      _, encoder_t,decoder=self.get_ckpt(epoch=epoch, time=args[\"time\"])\n",
        "\n",
        "      for idx,batch in enumerate(self.test_loader): \n",
        "          image=batch[0].float().cuda()\n",
        "          image_name=batch[1]\n",
        "          # label=label.view(label.shape[0],1)\n",
        "\n",
        "          x1,x2,x3,x4,x5 = encoder_t(image)\n",
        "          pred = decoder(x1,x2,x3,x4,x5)\n",
        "\n",
        "          #print original image\n",
        "          img2=image[0]*255\n",
        "          digitized_img_t=np.transpose(img2.detach().cpu().numpy(),(1,2,0))\n",
        "          cv2_imshow(digitized_img_t)\n",
        "\n",
        "          #print grey image[0]\n",
        "          img1=pred[0]*255\n",
        "          grey_img_t=np.transpose(img1.detach().cpu().numpy(),(1,2,0))\n",
        "          cv2_imshow(grey_img_t)\n",
        "\n",
        "          #print binary image[1]\n",
        "          pred1=pred.detach()\n",
        "          pred1[pred1>0.5]=1\n",
        "          pred1[pred1<=0.5]=0\n",
        "          \n",
        "          img0=pred1[0]*255\n",
        "          binary_img_t=np.transpose(img0.detach().cpu().numpy(),(1,2,0))\n",
        "          cv2_imshow(binary_img_t)\n",
        "\n",
        "          # save_image(img, '/content/drive/My Drive/FYP/ECGpdf2data/data/test/pred_images_s/idx_'+epoch+'_'+str(iter)+'_pred.png')\n",
        "          # os.makedirs('/content/drive/My Drive/FYP_Yixin_Cai/GAA_data/train/output2/binary_image_'+args[\"time\"], exist_ok=True)\n",
        "          if path.exists('/content/drive/My Drive/FYP_Yixin_Cai/GAA_data/test/output2/binary_image_') == False:\n",
        "            os.mkdir('/content/drive/My Drive/FYP_Yixin_Cai/GAA_data/test/output2/binary_image_')\n",
        "          if path.exists('/content/drive/My Drive/FYP_Yixin_Cai/GAA_data/test/output2/greyscale_image_') == False:\n",
        "            os.mkdir('/content/drive/My Drive/FYP_Yixin_Cai/GAA_data/test/output2/greyscale_image_')\n",
        "          for i in range(0,image.shape[0]):\n",
        "            cv2.imwrite('/content/drive/My Drive/FYP_Yixin_Cai/GAA_data/test/output2/binary_image_'+'/'\n",
        "                                          +str(image_name[i])+'.png', binary_img_t)\n",
        "            cv2.imwrite('/content/drive/My Drive/FYP_Yixin_Cai/GAA_data/test/output2/greyscale_image_'+'/'\n",
        "                        +str(image_name[i])+'.png', grey_img_t)\n",
        "              \n",
        "\n",
        "      print(\"GAA test complete.\")\n",
        "      # if (accuracy/(args['batch_size']*len(self.test_loader)))<0.5:\n",
        "      #    print(\"test accuracy: \",0.621488,\" over \", (args['batch_size']*len(self.test_loader)))\n",
        "      # else:\n",
        "      #   print(\"ftest accuracy: \", accuracy/(args['batch_size']*len(self.test_loader)),\" over \", (args['batch_size']*len(self.test_loader)))\n",
        " \n",
        "    def model_eval(self,encoder_s,decoder,seg_loss):\n",
        "        print(\"----------Start validating model----------\")\n",
        "\n",
        "        encoder_s.eval()\n",
        "        decoder.eval()\n",
        "        # cnn_model.eval()\n",
        "\n",
        "        seg_losses=[]\n",
        "        for idx,batch in enumerate(self.val_loader):\n",
        "\n",
        "            image_s = batch[0].float().cuda()\n",
        "            label_s = batch[1].float().cuda()\n",
        "            image_t = batch[2].float().cuda() \n",
        "            # label=label.view(label.shape[0],1)\n",
        "\n",
        "\n",
        "            x1,x2,x3,x4,x5 = encoder_s(image_s)\n",
        "            pred = decoder(x1,x2,x3,x4,x5)\n",
        "            loss_seg = seg_loss(pred,label_s)\n",
        "            \n",
        "            seg_losses.append(loss_seg.item())\n",
        "        \n",
        "        return mean(seg_losses)\n",
        "\n",
        "          \n",
        "        # return np.average(val_loss_cnn),total_correct/(len(self.test_loader)*args['batch_size'])\n",
        "\n",
        "    def train(self,retrain=0):\n",
        "        encoder_s = Encoder(3).cuda() #source encoder\n",
        "        encoder_t = Encoder(3).cuda() #target encoder\n",
        "        decoder = Decoder(1).cuda() #6\n",
        "        if retrain==1:\n",
        "           encoder_s,encoder_t,decoder=self.get_ckpt(epoch=1,time=1)\n",
        "        # cnn_model=CNN().cuda()\n",
        "        # encoder_s,encoder_t,decoder,cnn_model=self.get_ckpt(40)\n",
        "        E_discriminator = Encoder_Discriminator(32).cuda() #discriminator for encoder #size=24\n",
        "        D_discriminator = Resnet34(1).cuda() #discriminator for decoder #6\n",
        "        optimizer_en_s = optim.Adam(encoder_s.parameters(),lr=args[\"lr\"],weight_decay=5e-3)#1e-2\n",
        "        optimizer_en_t = optim.Adam(encoder_t.parameters(),lr=args[\"lr\"],weight_decay=5e-3)\n",
        "        optimizer_de = optim.Adam(decoder.parameters(),lr=args[\"lr\"],weight_decay=1e-2)\n",
        "        optimizer_ED = optim.Adam(E_discriminator.parameters(),lr=args[\"lr_ed\"],weight_decay=5e-3)\n",
        "        optimizer_DD = optim.Adam(D_discriminator.parameters(),lr=args[\"lr_ed\"],weight_decay=5e-4)\n",
        "        # optimizer_cnn = optim.Adam(cnn_model.parameters(),lr=args[\"lr_cnn\"])#,weight_decay=5e-4)\n",
        "\n",
        "        seg_loss =nn.BCELoss().cuda()\n",
        "        E_loss = nn.BCELoss().cuda()\n",
        "        D_loss = nn.BCELoss().cuda()\n",
        "        C_loss = nn.BCELoss().cuda()\n",
        "        # C_loss=nn.BCEWithLogitsLoss().cuda()\n",
        "        # toal_loss= \n",
        "\n",
        "        print(\"----------start training----------\")\n",
        "\n",
        "        iters = 1\n",
        "        iou = 0.\n",
        "        dice = 0.\n",
        "        acc = 0.\n",
        "        pre = 0.\n",
        "        sen = 0.\n",
        "        f1 = 0. \n",
        "        accuracy_val=[]\n",
        "\n",
        "        # to track the training loss as the model trains\n",
        "        train_losses_seg = []\n",
        "        train_losses_cnn = []\n",
        "        train_dd_loss=[]\n",
        "        # to track the validation loss as the model trains\n",
        "        valid_losses_seg = []\n",
        "        valid_losses_cnn = []\n",
        "        # to track the average training loss per epoch as the model trains\n",
        "        avg_train_losses_cnn = []\n",
        "        avg_train_losses_seg = []\n",
        "        # to track the average validation loss per epoch as the model trains\n",
        "        avg_valid_losses_correctness = [] \n",
        "        avg_valid_losses_acc = [] \n",
        "        avg_valid_losses_f1 = [] \n",
        "        avg_valid_losses_sen = [] \n",
        "        avg_valid_losses_pre = [] \n",
        "        avg_valid_losses_cnn = [] \n",
        "        avg_train_dd_loss=[]\n",
        "        val_losses=[]\n",
        "\n",
        "\n",
        "\n",
        "        \n",
        "        best_acc=args[\"best_acc\"]\n",
        "        es=0\n",
        "\n",
        "        \n",
        "        batch=0\n",
        "        for epoch in range(args[\"epoches\"]):\n",
        "            encoder_s.train()\n",
        "            encoder_t.train()\n",
        "            decoder.train()\n",
        "            E_discriminator.train()\n",
        "            D_discriminator.train()\n",
        "            # cnn_model.train()\n",
        "\n",
        "\n",
        "            ######################    \n",
        "            # train the model #\n",
        "            ######################\n",
        "                        \n",
        "            for idx,batch in enumerate(self.train_loader):\n",
        "                print(\"************\",idx,\"*************\")\n",
        "\n",
        "                image_s = batch[0].cuda()\n",
        "                label_s = batch[1].cuda()\n",
        "                image_t = batch[2].cuda()\n",
        "\n",
        "                e_real = torch.full((image_s.shape[0],1),1).float().cuda() #4 rows x 1 columns\n",
        "                e_fake = torch.full((image_s.shape[0],1),0).float().cuda()\n",
        "                d_real = torch.full((image_s.shape[0],1,4,4),1).float().cuda()\n",
        "                d_fake = torch.full((image_s.shape[0],1,4,4),0).float().cuda()\n",
        "                c_real = torch.full((image_s.shape[0],1),1).float().cuda()\n",
        "                optimizer_en_s.zero_grad()\n",
        "                optimizer_de.zero_grad()\n",
        "\n",
        "                x1,x2,x3,x4,x5 = encoder_s(image_s)\n",
        "\n",
        "                pred = decoder(x1,x2,x3,x4,x5)\n",
        "\n",
        "#print out pred image\n",
        "               #greyscale image\n",
        "                # img1 = pred[0].detach()#.cpu().numpy() #torch.Size([3,28,28]\n",
        "                # img1=img1*255\n",
        "                # # print(img1)\n",
        "                # grey_img=np.transpose(img1.detach().cpu().numpy(),(1,2,0))\n",
        "                # cv2_imshow(grey_img)\n",
        "\n",
        "                # #binary image\n",
        "                # pred1=pred.detach()\n",
        "                # pred1[pred1>0.5]=1\n",
        "                # pred1[pred1<=0.5]=0\n",
        "                # img4 = pred1[0]*255 #.detach().cpu().numpy() #torch.Size([3,28,28]\n",
        "                # binary_img=np.transpose(img4.detach().cpu().numpy(),(1,2,0))\n",
        "                # cv2_imshow(binary_img)\n",
        "\n",
        "                  \n",
        "                #loss\n",
        "                loss_seg = seg_loss(pred,label_s)\n",
        "                train_losses_seg.append(loss_seg.item())\n",
        "                # loss_seg = seg_loss(pred, torch.squeeze(label_s))\n",
        "                optimizer_en_s.step()\n",
        "                optimizer_de.step()\n",
        "\n",
        "                print(\"[{0:d}:{1:d}] --- loss_seg:{2:.10f}\".format(epoch + 1,iters,loss_seg.item()))\n",
        "                \n",
        "                if (epoch+1) >= 1:\n",
        "                    param_s = list(encoder_s.parameters())\n",
        "                    param_t = list(encoder_t.parameters())\n",
        "                    param_copy(param_s,param_t,0.8,epoch)\n",
        "\n",
        "                    if iters % 2 == 1:\n",
        "                        optimizer_ED.zero_grad()\n",
        "                        _,_,_,_,s5 = encoder_s(image_s)\n",
        "                        # print(\"train, s5 \", s5.shape,type(s5))#torch.Size([4, 512, 32, 32]) <class 'torch.Tensor'>\n",
        "                        encoder_pred_s = E_discriminator(s5)\n",
        "                        en_d_s_loss = E_loss(encoder_pred_s,e_real)\n",
        "                        en_d_s_loss.backward()\n",
        "\n",
        "                        _,_,_,_,t5 = encoder_t(image_t)\n",
        "                        encoder_pred_t = E_discriminator(t5)\n",
        "                        en_d_t_loss = E_loss(encoder_pred_t,e_fake)\n",
        "                        en_d_t_loss.backward()\n",
        "                        optimizer_ED.step()\n",
        "\n",
        "                    optimizer_en_s.zero_grad()\n",
        "                    optimizer_en_t.zero_grad()\n",
        "                    optimizer_ED.zero_grad()\n",
        "                    _,_,_,_,t5 = encoder_t(image_t)     \n",
        "                    encoder_pred_t = E_discriminator(t5)\n",
        "                    en_g_t_loss = E_loss(encoder_pred_t,e_real)\n",
        "                    en_g_t_loss.backward()\n",
        "                    optimizer_en_t.step()\n",
        "\n",
        "                    if iters % 4 == 1:\n",
        "                        # optimizer_DD.zero_grad()\n",
        "                        # optimizer_cnn.zero_grad()\n",
        "                        optimizer_DD.zero_grad()\n",
        "                        s1,s2,s3,s4,s5 = encoder_s(image_s)\n",
        "                        decoder_s = decoder(s1,s2,s3,s4,s5)\n",
        "                        decoder_pred_s = D_discriminator(decoder_s)\n",
        "                        de_d_s_loss = D_loss(decoder_pred_s,d_real)\n",
        "                        de_d_s_loss.backward()\n",
        "                        \n",
        "                        t1,t2,t3,t4,t5 = encoder_t(image_t)\n",
        "                        decoder_t = decoder(t1,t2,t3,t4,t5)\n",
        "\n",
        "                        #cnn \n",
        "                        # optimizer_cnn.zero_grad()\n",
        "                        # cnn_pred=decoder_t.detach().clone().cuda()\n",
        "                        # cnn_pred1=cnn_pred.detach()\n",
        "                        # cnn_pred1[cnn_pred1>0.5]=1\n",
        "                        # cnn_pred1[cnn_pred1<=0.5]=0\n",
        "                      \n",
        "                        # cnn_output=cnn_model(cnn_pred1)\n",
        "                        \n",
        "                        # loss_cnn=C_loss(cnn_output,image_t_label)#*0.1\n",
        "                        # if(loss_cnn>1):\n",
        "                        #   print(\"cnnLoss exceeds : \",cnn_output,image_t_label)\n",
        "                        #   loss_cnn= loss_cnn*0.1\n",
        "                        # train_losses_cnn+=loss_cnn.item()#*args[\"batch_size\"]\n",
        "                        # loss_cnn.backward()\n",
        "                        # optimizer_cnn.step()\n",
        "\n",
        "                        # train_losses_cnn+=loss_cnn.item()*args[\"batch_size\"]\n",
        "                        # train_losses_cnn.append(loss_cnn.item())\n",
        "\n",
        "                        decoder_pred_t = D_discriminator(decoder_t)\n",
        "                        # print(\"decoder_pred_t:\",decoder_pred_t.shape,type(decoder_pred_t))\n",
        "                        de_d_t_loss = D_loss(decoder_pred_t,d_fake)\n",
        "                        de_d_t_loss.backward()\n",
        "                        optimizer_DD.step()\n",
        "\n",
        "                        optimizer_en_t.zero_grad()\n",
        "                        optimizer_de.zero_grad()\n",
        "                        t1,t2,t3,t4,t5 = encoder_t(image_t)\n",
        "                        decoder_t = decoder(t1,t2,t3,t4,t5)\n",
        "                        decoder_pred_t = D_discriminator(decoder_t)\n",
        "                        de_g_t_loss = D_loss(decoder_pred_t,d_real) \n",
        "                        de_g_t_loss.backward()\n",
        "                        optimizer_en_t.step()\n",
        "\n",
        "                        #greyscale image of target ds\n",
        "                        # decoder_t1= decoder_t.detach()\n",
        "                        # img1 = decoder_t1[0]*255 #.detach().cpu().numpy() #torch.Size([3,28,28]\n",
        "                        # # print(img1)\n",
        "                        # grey_img_t=np.transpose(img1.detach().cpu().numpy(),(1,2,0))\n",
        "                        # cv2_imshow(grey_img_t)\n",
        "\n",
        "                        #binary image of target ds\n",
        "                        # decoder_t1[decoder_t1>0.5]=1\n",
        "                        # decoder_t1[decoder_t1<=0.5]=0\n",
        "                        # img1 = decoder_t1[0]*255 #.detach().cpu().numpy() #torch.Size([3,28,28]\n",
        "                        # binary_img_t=np.transpose(img1.detach().cpu().numpy(),(1,2,0))\n",
        "                        # cv2_imshow(binary_img_t)\n",
        "\n",
        "                        # cv2.imwrite('/content/drive/My Drive/FYP/ECGpdf2data/data/train/output/binary_image_t2/'\n",
        "                        #                 +str(epoch)+'_'+str(iters)+'_.png', binary_img_t)\n",
        "                        # cv2.imwrite('/content/drive/My Drive/FYP/ECGpdf2data/data/train/output/greyscale_image_t2/'\n",
        "                        # +str(epoch)+'_'+str(iters)+'_.png', grey_img_t)\n",
        "                    print(\"[{0:d}:{1:d}] --- ED_loss_s:{2:.8f}\\tED_loss_t:{3:.8f}\\tEG_loss_t:{4:.8f}\\tDD_loss_s:{5:.8f}\\tDD_loss_t:{6:.8f}\\tDG_loss_t:{7:.8f}\".format(\n",
        "                            epoch + 1,iters,en_d_s_loss.item(),en_d_t_loss.item(),en_g_t_loss.item(),\n",
        "                            de_d_s_loss.item(),de_d_t_loss.item(),de_g_t_loss.item()))\n",
        "                        # print(\"[{0:d}:{1:d}] --- loss_CNN:{2:.10f}\".format(\n",
        "                    \n",
        "                            # epoch + 1,iters,loss_cnn.item()))\n",
        "                  \n",
        "                    train_dd_loss.append(de_d_s_loss.item())        \n",
        "                iters += 1\n",
        "\n",
        "            # train_loss_seg = np.average(train_losses_seg)\n",
        "                # avg_train_losses_seg.append(train_loss_seg)\n",
        "            \n",
        "                # train_loss_dd=np.average(train_dd_loss)\n",
        "                # avg_train_dd_loss.append(train_loss_dd)\n",
        "                # train_dd_loss=[]\n",
        "            avg_train_losses_seg.append(np.average(train_losses_seg))\n",
        "            train_losses_seg=[]\n",
        "            # train_loss_cnn = np.average(train_losses_cnn)\n",
        "            # avg_train_losses_cnn.append(train_loss_cnn)\n",
        "                # avg_train_dd_loss.append(train_loss_dd)\n",
        "\n",
        "            # train_losses_seg=[]\n",
        "            # train_dd_loss=[]\n",
        "            #,train_losses_cnn = [],[]\n",
        "                # valid_losses = []\n",
        "                \n",
        "            #     Loss_seg.append(loss_seg.item())\n",
        "            #     Loss_CNN.append(loss_cnn.item())\n",
        "\n",
        "            #     writer.add_scalar(\"SegLoss/train_batch\", loss_seg.item(), iters)\n",
        "            #     writer.add_scalar(\"CNNLoss/train_batch\", loss_cnn.item(), iters)\n",
        "            #     # writer.add_scalar(\"Pre\", acc, epoch)\n",
        "            #     # writer.add_scalar(\"F1\", acc, epoch)\n",
        "            #     # iters += 1\n",
        "\n",
        "            # writer.add_scalar(\"SegLoss/train_epoch\", np.mean(Loss_seg), epoch)\n",
        "            # writer.add_scalar(\"CNNLoss/train_epoch\", np.mean(Loss_CNN),epoch)\n",
        "\n",
        "\n",
        "            ######################    \n",
        "            # validate the model #\n",
        "            ######################\n",
        "            \n",
        "            val_loss=self.model_eval(encoder_s,decoder,seg_loss) #loss_cnn,total_correct\n",
        "            val_losses.append(val_loss)\n",
        "            self.save_ckpt(encoder_s,encoder_t,decoder,epoch+1)\n",
        "            # print(\"[0:\"+str(epoch+1)+\":validation] --- loss_seg:\"+val_loss+\"\\t\")\n",
        "            print(\"[{0:d}:validation] --- loss_seg:{1:.10f}\".format(epoch + 1,val_loss))\n",
        "\n",
        "            # accuracy_val.append(accuracy)\n",
        "\n",
        "            # if epoch==0:\n",
        "              # best_acc=total_correct\n",
        "            # writer.add_scalar(\"Loss\", total_loss, epoch)\n",
        "            # writer.add_scalar(\"Accuracy/val\", acc, epoch)\n",
        "            # writer.add_scalar(\"Sen/val\", acc, epoch)\n",
        "            # writer.add_scalar(\"Pre/val\", acc, epoch)\n",
        "            # writer.add_scalar(\"F1/val\", acc, epoch)\n",
        "            # writer.add_scalar(\"Accuracy\", total_correct/ len(train_set), epoch)\n",
        "            # test_iou,test_dice,acc,sen,pre,f1, auc= self.model_eval(encoder_t,decoder,cnn_model)\n",
        "            # print(test_iou,test_dice)\n",
        "            # if f1>0:#(test_iou>iou) & (test_dice>dice):\n",
        "            # if  epoch ==args[\"epoches\"]:\n",
        "            #   cv2.imwrite(\"/content/drive/My Drive/FYP/ECGpdf2data/data/train/binary_image/binary_image.png\", binary_image)\n",
        "            #   cv2.imwrite(\"/content/drive/My Drive/FYP/ECGpdf2data/data/train/greyscale_image/greyscale_image.png\", grey_image)\n",
        "            #   cv2.imwrite(\"/content/obj_measurement_new.jpg\", resized)\n",
        "            # self.save_ckpt(encoder_s,encoder_t,decoder,epoch+1)\n",
        "            self.model_test(epoch+1)\n",
        "            args[\"time\"]= args[\"time\"]+1\n",
        "                # iou = test_iou\n",
        "                # dice = test_dice\n",
        "            # if total_correct > best_acc:\n",
        "            #   best_acc = total_correct\n",
        "            #   es = 0\n",
        "            #   self.save_ckpt(encoder_s,encoder_t,decoder,cnn_model,epoch)\n",
        "            #   # torch.save(net.state_dict(), \"model_\" + str(fold) + 'weight.pt')\n",
        "            # else:\n",
        "            #     es += 1\n",
        "            #     print(\"Counter {} of 5\".format(es),\"total_correct: \", total_correct)\n",
        "\n",
        "            #     if es > 4:\n",
        "            #       print(\"Early stopping with best_acc: \", best_acc, \"and val_acc for this epoch: \", total_correct, \"...\")\n",
        "            #       break\n",
        "            #       # writer.flush()\n",
        "            #       # writer.close()  \n",
        "\n",
        "        # plot_loss(avg_train_losses_seg,\"Loss_seg_train\")\n",
        "        # # val_losses\n",
        "        # plot_loss(val_losses,\"Validation loss\")\n",
        "        # plot_loss(avg_train_dd_loss,\"Loss_seg_dd_s_train\")\n",
        "        # plot_loss(avg_valid_losses_cnn,'avg_valid_losses_cnn')\n",
        "        # plot_loss(accuracy_val,'accuracy')\n",
        "        # self.model_test(encoder_t,decoder,cnn_model)\n",
        "        # return {'avg_train_losses_seg': avg_train_losses_seg,'avg_train_losses_dds': avg_train_dd_loss}#,\n",
        "               # 'auuracy_val':accuracy_val,'avg_train_losses_cnn': avg_train_losses_cnn}\n",
        "        # return {'avg_train_losses_seg': avg_train_losses_seg,'avg_train_losses_cnn': avg_train_losses_cnn,\n",
        "        #         'avg_valid_losses_acc': avg_valid_losses_acc,'avg_valid_losses_f1': avg_valid_losses_f1,\n",
        "        #         'avg_valid_losses_correctness':avg_valid_losses_correctness,'avg_valid_losses_cnn': avg_valid_losses_cnn,\n",
        "        #         'avg_train_losses_dds': avg_train_dd_loss}\n",
        "        ###plot\n",
        "        return avg_train_losses_seg,val_losses\n",
        "        \n",
        "    def train(self):\n",
        "      cnn_model=CNN().cuda()\n",
        "\n",
        "      optimizer_cnn = optim.Adam(cnn_model.parameters(),lr=1e-03)#,weight_decay=5e-4)\n",
        "      C_loss = nn.BCELoss().cuda()\n",
        "\n",
        "      ###training the model###\n",
        "      print('*******CNN training********')\n",
        "      train_losses_avg=[]\n",
        "      train_acc_avg=[]\n",
        "\n",
        "      cnn_model.train()\n",
        "      # print(len(self.train_loader))\n",
        "      for epoch in range(args[\"epoches\"]):\n",
        "        \n",
        "        train_losses = []\n",
        "        train_acc=0\n",
        "        iter=0\n",
        "        for idx,batch in enumerate(self.train_loader):\n",
        "          iter+=1\n",
        "          image_t=batch[2].cuda()\n",
        "          image_t_label=batch[3].float().cuda()\n",
        "\n",
        "          cnn_output=cnn_model(image_t)\n",
        "\n",
        "          #loss\n",
        "          cnn_output=torch.flatten(cnn_output).cuda()\n",
        "          # print(cnn_output,image_t_label)\n",
        "          loss_cnn=C_loss(cnn_output,image_t_label)\n",
        "          train_losses.append(loss_cnn.item())\n",
        "          \n",
        "          optimizer_cnn.zero_grad()\n",
        "          loss_cnn.backward()\n",
        "          optimizer_cnn.step()\n",
        "          \n",
        "          # accuracy#\n",
        "          train_acc += get_num_correct(cnn_output,image_t_label)\n",
        "          # print(train_acc)\n",
        "        \n",
        "        num=(iter)*args[ \"batch_size\"]\n",
        "        train_losses_avg.append(mean(train_losses))\n",
        "        train_acc_avg.append(train_acc/num)\n",
        "        print('Epoch : ',epoch+1, '\\t', 'loss :',  mean(train_losses_avg), \n",
        "              'acc :',mean(train_losses) ,num)\n",
        "\n",
        "      plt.plot( train_losses_avg, label='Training loss')\n",
        "      plt.show()\n",
        "      plt.plot( train_acc_avg, label='Training Accuracy')\n",
        "      plt.show()\n",
        "      plt.plot( train_losses_avg, label='Training loss')\n",
        "      plt.plot( train_acc_avg, label='Training Accuracy')\n",
        "      plt.show()\n",
        "      # plt.legend()\n",
        "      \n",
        "\n",
        "      ###testing the model###\n",
        "      print('*******CNN training********')\n",
        "      cnn_model.eval()\n",
        "      test_losses_avg=[]\n",
        "      test_losses = []\n",
        "      test_acc=0\n",
        "      # print(len(self.test_loader))\n",
        "      iter=0\n",
        "      for idx,batch in enumerate(self.test_loader):\n",
        "        iter+=1\n",
        "        image = batch[0].float().cuda()\n",
        "        label = batch[1].float().cuda().float().cuda()\n",
        "\n",
        "        cnn_output=cnn_model(image)\n",
        "        cnn_output=torch.flatten(cnn_output).cuda()\n",
        "        # print(cnn_output,label)\n",
        "        loss_cnn=C_loss(cnn_output,label)\n",
        "        test_losses.append(loss_cnn.item())\n",
        "        # accuracy#\n",
        "        test_acc += get_num_correct(cnn_output,label)\n",
        "        # print(test_acc)\n",
        "      num=(iter)*args[ \"batch_size\"]\n",
        "      plt.plot( test_losses, label='Test loss')\n",
        "      # plt.legend()\n",
        "      plt.show()\n",
        "      print('Test loss average : ', mean(test_losses),'Test Acuracy : ', test_acc/num,num)"
      ],
      "metadata": {
        "id": "AsJDr4dP8leC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##CNN_Train"
      ],
      "metadata": {
        "id": "uzABWMEx-Ogv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import random_split\n",
        "\n",
        "class CNN_Trainers:\n",
        "    def __init__(self):\n",
        "        data_loader = Choroid_loader(root_path=args[\"root_CNN\"],datasets=\"CNN\")\n",
        "        self.train_loader = data_loader.load_train_data(batch_size=args[\"cnn_batch_size\"])\n",
        "        self.val_loader = data_loader.load_val_data(batch_size=args[\"cnn_batch_size\"])\n",
        "        self.test_loader = data_loader.load_test_data(batch_size=args['cnn_batch_size'])\n",
        "        # print(len(self.train_loader.dataset))\n",
        "    def save_ckpt(self,cnn_model,epoch):\n",
        "        if not os.path.exists(args[\"ckpt_path2\"]):\n",
        "            os.makedirs(args[\"ckpt_path2\"])\n",
        "\n",
        "        state = {\"cnn_model\":cnn_model.state_dict()}\n",
        "        torch.save(state,args[\"ckpt_path2\"] + args[\"name\"] + \"_epoch_\" + str(epoch) + \".pkl\")\n",
        "        print(\"---> save model:{} <---\".format(args[\"ckpt_path2\"]))\n",
        "        print(args[\"ckpt_path2\"] + args[\"name\"] + \"_epoch_\" + str(epoch) +\".pkl\")\n",
        "\n",
        "    def get_ckpt(self,epoch):\n",
        "        cnn_model=CNN().cuda()\n",
        "\n",
        "        checkpoint = torch.load(args[\"ckpt_path2\"] + args[\"name\"] + \"_epoch_\" + str(epoch)+ \".pkl\")\n",
        "        cnn_model.load_state_dict(checkpoint['cnn_model'])\n",
        "        return cnn_model    \n",
        "\n",
        "    def test(self,epoch):#,cnn_model):\n",
        "      print(\"----------Start testing CNN model----------\")\n",
        "      accuracy=0\n",
        "      correct=0\n",
        "      # cnn_model=CNN().cuda()\n",
        "      cnn_model=self.get_ckpt(epoch)\n",
        "\n",
        "      for idx,batch in enumerate(self.test_loader): \n",
        "          image=batch[0].float().cuda()\n",
        "          label=batch[1].float().cuda()\n",
        "          label=label.view(label.shape[0],1)\n",
        "\n",
        "          pred = cnn_model(image)\n",
        "\n",
        "          pred[pred<=0.5]=0\n",
        "          pred[pred>0.5]=1\n",
        "            \n",
        "          # print(\"CNN output: \",cnn_output,\"Label: \",label)\n",
        "          # accuracy_score(train_y, predictions)\n",
        "          correct += (pred == label).float().sum().item()\n",
        "          \n",
        "          # accurate=get_num_correct(cnn_output,label)\n",
        "          # print(type(accurate),accurate)\n",
        "          # accuracy+=accurate\n",
        "      accuracy = correct / len(self.test_loader.dataset)\n",
        "\n",
        "      # plot_loss(accuracy,'Test Acc')\n",
        "      # plot_loss(accuracy,'Test Accuracy')\n",
        "      # if (accuracy/(args['batch_size']*len(self.test_loader)))<0.5:\n",
        "      #    print(\"test accuracy: \",0.621488,\" over \", (args['batch_size']*len(self.test_loader)))\n",
        "      # else:\n",
        "      # print(\"test accuracy: \", accuracy/(args['batch_size']*len(self.test_loader)),\" over \", (args['batch_size']*len(self.test_loader)))\n",
        "      print(\"Test Accuracy = {}\".format(accuracy))\n",
        " \n",
        "    def eval(self,cnn_model,C_loss):\n",
        "        print(\"----------Start validating CNN model----------\")\n",
        "\n",
        "        cnn_model.eval()\n",
        "\n",
        "        iou,dice = [],[]\n",
        "        acc,sen,pre,f1, auc= [],[],[],[],[]\n",
        "        val_loss_cnn=[]\n",
        "        file_num = 0\n",
        "        total_correct=0\n",
        "\n",
        "        correct=0\n",
        "        accuracy=0\n",
        "        train_losses=[]\n",
        "        for idx,batch in enumerate(self.val_loader):\n",
        "            image = batch[0].float().cuda()\n",
        "            label = batch[1].float().cuda()\n",
        "            label=label.view(label.shape[0],1)\n",
        "          \n",
        "            pred = cnn_model(image)\n",
        "            loss_cnn=C_loss(pred,label)\n",
        "            train_losses.append(loss_cnn.item())\n",
        "\n",
        "            # print(\"CNN val output: \",pred,\"Label: \",label)\n",
        "            # accuracy_score(train_y, predictions)\n",
        "            pred[pred>0.5]=1\n",
        "            pred[pred<=0.5]=0\n",
        "            correct += (pred == label).float().sum().item()\n",
        "\n",
        "            # total_correct+=get_num_correct(cnn_output,label)\n",
        "            # print(\"correct: \",total_correct)\n",
        "            # file_num += 1 \n",
        "\n",
        "        accuracy = correct / len(self.val_loader.dataset)\n",
        "        return mean(train_losses),accuracy\n",
        "\n",
        "        \n",
        "\n",
        "        # return np.mean(iou),np.mean(dice),np.mean(acc),np.mean(sen),np.mean(pre),np.mean(f1),np.mean(auc)\n",
        "        # pyplot.subplot(1, 2, 1) # have two plots in 1 row two columns, first plot\n",
        "        # assuming im is batch x channel x h x w and channel is RGB\n",
        "        # pyplot.plot(image[0,:,:,:].detach().cpu())#.permute(1, 2, 0))\n",
        "        # pyplot.subplot(1, 2, 2) # second plot\n",
        "        # pyplot.plot(pred[0,:,:,:].detach().cpu())\n",
        "\n",
        "        # for i in range(image.size(0)):\n",
        "        #   print(\"print image\")\n",
        "        #   img_plot = image[i,:,:,:].detach().cpu().permute(1, 2, 0)\n",
        "        #   plt.imshow(img_plot)\n",
        "        # for i in range(pred.size(0)):\n",
        "        #   print(\"print pred\")\n",
        "        #   img_plot = pred[i,:,:,:].detach().cpu().permute(1, 2, 0)\n",
        "        #   # img_plot.squeeze(axis=2)\n",
        "        #   plt.imshow(img_plot.squeeze(axis=2))\n",
        "        \n",
        "        \n",
        "        # return np.mean(acc),np.mean(sen),np.mean(pre),np.mean(f1),total_correct/len(self.test_loader),np.average(val_loss_cnn)#/len(self.test_loader)#,np.mean(auc)\n",
        "        \n",
        "        # return np.average(val_loss_cnn),total_correct/(len(self.test_loader)*args['batch_size'])\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "          cnn_model=CNN().cuda()\n",
        "\n",
        "          optimizer_cnn = optim.Adam(cnn_model.parameters(),lr=1e-03)#,weight_decay=5e-4)\n",
        "          C_loss = nn.BCELoss().cuda()\n",
        "\n",
        "          ###training the model###\n",
        "          print('*******CNN training********')\n",
        "          train_losses_avg=[]\n",
        "          val_losses_avg=[]\n",
        "          # train_acc_avg=[]\n",
        "          train_accuracy=[]\n",
        "          val_accuracy=[]\n",
        "\n",
        "          cnn_model.train()\n",
        "          # print(len(self.train_loader))\n",
        "          for epoch in range(args[\"epoches\"]):\n",
        "            \n",
        "            train_losses = []\n",
        "            \n",
        "            # train_acc=0\n",
        "            # iter=0\n",
        "\n",
        "            correct=0\n",
        "            for idx,batch in enumerate(self.train_loader):\n",
        "              image = batch[0].float().cuda()\n",
        "              label = batch[1].float().cuda()\n",
        "              label=label.view(label.shape[0],1)\n",
        "              # print(image.shape[0])\n",
        "\n",
        "              pred=cnn_model(image)\n",
        "              # pred=torch.flatten(pred).cuda()\n",
        "              # print(\"CNN train output: \",pred,\"Label: \",label)\n",
        "# \n",
        "              #loss\n",
        "              # pred=torch.flatten(pred).cuda()\n",
        "              # print(cnn_output,image_t_label)\n",
        "              loss_cnn=C_loss(pred,label)\n",
        "              # print(loss_cnn)\n",
        "              train_losses.append(loss_cnn.item())\n",
        "              \n",
        "              optimizer_cnn.zero_grad()\n",
        "              loss_cnn.backward()\n",
        "              optimizer_cnn.step()\n",
        "              \n",
        "              # accuracy#\n",
        "              pred[pred>0.5]=1\n",
        "              pred[pred<=0.5]=0\n",
        "              correct += (pred == label).float().sum().item()\n",
        "              # print(\"correct:\", correct)\n",
        "              # train_acc += get_num_correct(cnn_output,image_t_label)\n",
        "              # print(train_acc)\n",
        "            \n",
        "            # num=(iter)*args[ \"batch_size\"]\n",
        "            train_losses_avg.append(mean(train_losses))\n",
        "            train_accuracy.append(correct / len(self.train_loader.dataset) )\n",
        "\n",
        "            # train_acc_avg.append(train_acc/num)\n",
        "            print('Epoch : ',epoch+1, '\\t', 'loss :',  mean(train_losses), \n",
        "                  'acc :',train_accuracy[-1])\n",
        "            correct=0\n",
        "            train_losses=0\n",
        "\n",
        "            ######################    \n",
        "            # validate the model #\n",
        "            ######################\n",
        "            val_loss,val_acc=self.eval(cnn_model,C_loss)\n",
        "            val_losses_avg.append(val_loss)\n",
        "            val_accuracy.append(val_acc)\n",
        "            print('Validation loss :',  val_loss, \n",
        "                  'acc :',val_acc)\n",
        "\n",
        "            if val_losses_avg[-1]<=0.6:\n",
        "                self.save_ckpt(cnn_model,epoch+1)\n",
        "\n",
        "          plot_loss(train_losses_avg,'Train loss',val_losses_avg,'Validation loss')\n",
        "          plot_loss(train_accuracy,'Train Acc',val_accuracy,'Validation Acc')\n",
        "          # plt.plot( train_losses_avg, label='Train loss')\n",
        "          # plt.plot( val_losses_avg, label='Validation loss')\n",
        "          # plt.show()\n",
        "          # plot_loss(train_accuracy,'Train Accuracy')\n",
        "          # plot_loss(val_accuracy,'Validation Accuracy')\n",
        "          # plt.show()\n",
        "\n",
        "          # plt.show()\n",
        "\n",
        "          \n",
        "          \n",
        "          # plt.legend()\n",
        "          \n",
        "\n",
        "          # ###testing the model###\n",
        "          # print('*******CNN training********')\n",
        "          # cnn_model.eval()\n",
        "          # test_losses_avg=[]\n",
        "          # test_losses = []\n",
        "          # test_acc=0\n",
        "          # # print(len(self.test_loader))\n",
        "          # iter=0\n",
        "          # for idx,batch in enumerate(self.test_loader):\n",
        "          #   iter+=1\n",
        "          #   image = batch[0].float().cuda()\n",
        "          #   label = batch[1].float().cuda().float().cuda()\n",
        "\n",
        "          #   cnn_output=cnn_model(image)\n",
        "          #   cnn_output=torch.flatten(cnn_output).cuda()\n",
        "          #   # print(cnn_output,label)\n",
        "          #   loss_cnn=C_loss(cnn_output,label)\n",
        "          #   test_losses.append(loss_cnn.item())\n",
        "          #   # accuracy#\n",
        "          #   test_acc += get_num_correct(cnn_output,label)\n",
        "          #   # print(test_acc)\n",
        "          # num=(iter)*args[ \"batch_size\"]\n",
        "          # plt.plot( test_losses, label='Test loss')\n",
        "          # # plt.legend()\n",
        "          # plt.show()\n",
        "          # print('Test loss average : ', mean(test_losses),'Test Acuracy : ', test_acc/num,num)\n",
        "\n",
        "\n",
        "          ####store point"
      ],
      "metadata": {
        "id": "h3psRvBK-Pl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Run"
      ],
      "metadata": {
        "id": "_Nc1wKOyAsdK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "trainer = Trainers()\n",
        "avg_train_losses_seg,val_losses=trainer.train(retrain=1) \n",
        "plot_loss(avg_train_losses_seg,\"Loss_seg_train\")\n",
        "        # val_losses\n",
        "plot_loss(val_losses,\"Validation loss\")\n",
        "# cnn_trainer = CNN_Trainers()\n",
        "# loss_results=trainer.train()\n",
        "#trainer.CNN_train()\n",
        "#cnn_trainer.CNN_train()"
      ],
      "metadata": {
        "id": "xf8FsTFMAtKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args[\"time\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rk3Cr9V2Gj0i",
        "outputId": "e1f15853-e2ed-4eaa-d671-0fb2691328c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-scpR5kSGjqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "# trainer = Trainers()\n",
        "cnn_trainer=CNN_Trainers()"
      ],
      "metadata": {
        "id": "gYJv2WWmVGwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Aws4j_Flvcn9",
        "outputId": "add052f1-a358-4379-b481-394657967d2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*******CNN training********\n",
            "Epoch :  1 \t loss : 3.479722087936742 acc : 0.5144927536231884\n",
            "----------Start validating CNN model----------\n",
            "Epoch :  2 \t loss : 0.8369335860147008 acc : 0.7536231884057971\n",
            "----------Start validating CNN model----------\n",
            "Epoch :  3 \t loss : 0.5886325801589659 acc : 0.7971014492753623\n",
            "----------Start validating CNN model----------\n",
            "Epoch :  4 \t loss : 0.5176160074770451 acc : 0.8043478260869565\n",
            "----------Start validating CNN model----------\n",
            "---> save model:/content/drive/My Drive/FYP_Yixin_Cai/CNN_data/checkpoint/ <---\n",
            "/content/drive/My Drive/FYP_Yixin_Cai/CNN_data/checkpoint/GAA_epoch_4.pkl\n",
            "Epoch :  5 \t loss : 0.5206449005220618 acc : 0.8043478260869565\n",
            "----------Start validating CNN model----------\n",
            "---> save model:/content/drive/My Drive/FYP_Yixin_Cai/CNN_data/checkpoint/ <---\n",
            "/content/drive/My Drive/FYP_Yixin_Cai/CNN_data/checkpoint/GAA_epoch_5.pkl\n",
            "Epoch :  6 \t loss : 0.49097320304385256 acc : 0.8043478260869565\n",
            "----------Start validating CNN model----------\n",
            "---> save model:/content/drive/My Drive/FYP_Yixin_Cai/CNN_data/checkpoint/ <---\n",
            "/content/drive/My Drive/FYP_Yixin_Cai/CNN_data/checkpoint/GAA_epoch_6.pkl\n",
            "Epoch :  7 \t loss : 0.5010354896741254 acc : 0.7971014492753623\n",
            "----------Start validating CNN model----------\n",
            "---> save model:/content/drive/My Drive/FYP_Yixin_Cai/CNN_data/checkpoint/ <---\n",
            "/content/drive/My Drive/FYP_Yixin_Cai/CNN_data/checkpoint/GAA_epoch_7.pkl\n",
            "Epoch :  8 \t loss : 0.5058011630816119 acc : 0.8043478260869565\n",
            "----------Start validating CNN model----------\n",
            "Epoch :  9 \t loss : 0.5357700298939433 acc : 0.7898550724637681\n",
            "----------Start validating CNN model----------\n",
            "---> save model:/content/drive/My Drive/FYP_Yixin_Cai/CNN_data/checkpoint/ <---\n",
            "/content/drive/My Drive/FYP_Yixin_Cai/CNN_data/checkpoint/GAA_epoch_9.pkl\n",
            "Epoch :  10 \t loss : 0.5377461439264672 acc : 0.8043478260869565\n",
            "----------Start validating CNN model----------\n",
            "---> save model:/content/drive/My Drive/FYP_Yixin_Cai/CNN_data/checkpoint/ <---\n",
            "/content/drive/My Drive/FYP_Yixin_Cai/CNN_data/checkpoint/GAA_epoch_10.pkl\n",
            "Epoch :  11 \t loss : 0.49810913843767984 acc : 0.8043478260869565\n",
            "----------Start validating CNN model----------\n",
            "---> save model:/content/drive/My Drive/FYP_Yixin_Cai/CNN_data/checkpoint/ <---\n",
            "/content/drive/My Drive/FYP_Yixin_Cai/CNN_data/checkpoint/GAA_epoch_11.pkl\n",
            "Epoch :  12 \t loss : 0.5136529984218734 acc : 0.8043478260869565\n",
            "----------Start validating CNN model----------\n",
            "---> save model:/content/drive/My Drive/FYP_Yixin_Cai/CNN_data/checkpoint/ <---\n",
            "/content/drive/My Drive/FYP_Yixin_Cai/CNN_data/checkpoint/GAA_epoch_12.pkl\n",
            "Epoch :  13 \t loss : 0.5463951815451894 acc : 0.8043478260869565\n",
            "----------Start validating CNN model----------\n",
            "---> save model:/content/drive/My Drive/FYP_Yixin_Cai/CNN_data/checkpoint/ <---\n",
            "/content/drive/My Drive/FYP_Yixin_Cai/CNN_data/checkpoint/GAA_epoch_13.pkl\n",
            "Epoch :  14 \t loss : 0.497384856321982 acc : 0.8043478260869565\n",
            "----------Start validating CNN model----------\n",
            "---> save model:/content/drive/My Drive/FYP_Yixin_Cai/CNN_data/checkpoint/ <---\n",
            "/content/drive/My Drive/FYP_Yixin_Cai/CNN_data/checkpoint/GAA_epoch_14.pkl\n",
            "Epoch :  15 \t loss : 0.512761728571994 acc : 0.8043478260869565\n",
            "----------Start validating CNN model----------\n",
            "---> save model:/content/drive/My Drive/FYP_Yixin_Cai/CNN_data/checkpoint/ <---\n",
            "/content/drive/My Drive/FYP_Yixin_Cai/CNN_data/checkpoint/GAA_epoch_15.pkl\n",
            "Epoch :  16 \t loss : 0.5029040835797787 acc : 0.8043478260869565\n",
            "----------Start validating CNN model----------\n",
            "---> save model:/content/drive/My Drive/FYP_Yixin_Cai/CNN_data/checkpoint/ <---\n",
            "/content/drive/My Drive/FYP_Yixin_Cai/CNN_data/checkpoint/GAA_epoch_16.pkl\n",
            "Epoch :  17 \t loss : 0.5094588111553874 acc : 0.8043478260869565\n",
            "----------Start validating CNN model----------\n",
            "---> save model:/content/drive/My Drive/FYP_Yixin_Cai/CNN_data/checkpoint/ <---\n",
            "/content/drive/My Drive/FYP_Yixin_Cai/CNN_data/checkpoint/GAA_epoch_17.pkl\n",
            "Epoch :  18 \t loss : 0.49370772019028664 acc : 0.8043478260869565\n",
            "----------Start validating CNN model----------\n",
            "---> save model:/content/drive/My Drive/FYP_Yixin_Cai/CNN_data/checkpoint/ <---\n",
            "/content/drive/My Drive/FYP_Yixin_Cai/CNN_data/checkpoint/GAA_epoch_18.pkl\n",
            "Epoch :  19 \t loss : 0.5199698702033076 acc : 0.8043478260869565\n",
            "----------Start validating CNN model----------\n",
            "---> save model:/content/drive/My Drive/FYP_Yixin_Cai/CNN_data/checkpoint/ <---\n",
            "/content/drive/My Drive/FYP_Yixin_Cai/CNN_data/checkpoint/GAA_epoch_19.pkl\n",
            "Epoch :  20 \t loss : 0.5070248272802148 acc : 0.8043478260869565\n",
            "----------Start validating CNN model----------\n",
            "---> save model:/content/drive/My Drive/FYP_Yixin_Cai/CNN_data/checkpoint/ <---\n",
            "/content/drive/My Drive/FYP_Yixin_Cai/CNN_data/checkpoint/GAA_epoch_20.pkl\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_trainer.test(20)"
      ],
      "metadata": {
        "id": "yVZsR-AkCA5K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "198aee82-7519-4ffc-a03a-7e45c9894cc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------Start testing CNN model----------\n",
            "Test Accuracy = 0.7058823529411765\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seg_loss =nn.BCELoss().cuda()\n",
        "encoder_s,encoder_t,decoder=trainer.get_ckpt(epoch=1, time=1)\n",
        "val_loss=trainer.model_eval(encoder_s,decoder,seg_loss)\n",
        "# trainer.model_test(epoch=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyw4kGLtQkj5",
        "outputId": "b855ef31-1b49-4fc4-da1b-57144ff2ddae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------Start validating model----------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epoch=1\n",
        "print(\"[{0:d}:validation] --- loss_seg:{1:.10f}\".format(epoch + 1,val_loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTPvMqgOcaYY",
        "outputId": "e2e9a15e-3a5b-41da-93f4-93b5046ca1e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2:validation] --- loss_seg:0.6863833904\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.model_test(epoch=1)"
      ],
      "metadata": {
        "id": "H7ajTcNmS3qH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "image_list = os.listdir('/content/drive/My Drive/FYP_Yixin_Cai/GAA_data/test/output2')\n",
        "image_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkIZvbih4w3h",
        "outputId": "e36f50fe-6209-4d1f-d7e7-2ff6991567c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['binary_image_1',\n",
              " 'binary_image_0',\n",
              " 'greyscale_image_0',\n",
              " 'binary_image_2',\n",
              " 'greyscale_image_1',\n",
              " 'greyscale_image_2',\n",
              " 'binary_image_5']"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "n_lEZ8Yx5SEv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6276bca-a8a1-4e74-af93-7f96b5836de2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Test"
      ],
      "metadata": {
        "id": "aNhVwf5pGxaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# image_path = '/content/drive/My Drive/FYP/ECGpdf2data/data/train/cropped_png/cropped'\n",
        "image_path = '/content/drive/My Drive/FYP/ECGpdf2data/data/train/digitized_paper'\n",
        "image_list = os.listdir(image_path)\n",
        "image_list = [x for x in image_list if '.DS_Store' not in x]\n",
        "image_list = [x for x in image_list if '.ipynb_checkpoints' not in x]"
      ],
      "metadata": {
        "id": "Vz_oEdG8FNFg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8bb91215-ad38-44bc-a7af-c5856db22d4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-ed0c04887597>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# image_path = '/content/drive/My Drive/FYP/ECGpdf2data/data/train/cropped_png/cropped'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mimage_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/My Drive/FYP/ECGpdf2data/data/train/digitized_paper'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mimage_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mimage_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimage_list\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'.DS_Store'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mimage_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimage_list\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'.ipynb_checkpoints'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/FYP/ECGpdf2data/data/train/digitized_paper'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(image_list),type(image_list[0])"
      ],
      "metadata": {
        "id": "zmK0GkSwF4mY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_list"
      ],
      "metadata": {
        "id": "hjRo2VvrnOYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_list= [x[0:-4] for x in image_list]\n",
        "image_list, type(image_list[0])"
      ],
      "metadata": {
        "id": "efDZGntxfSvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_path_3 = '/content/drive/My Drive/FYP/ECGpdf2data/dataset/12_lead_ECG/test'\n",
        "image_list_3 = os.listdir(image_path_3)\n",
        "image_list_3  = [x for x in image_list_3  if '.DS_Store' not in x]\n",
        "image_list_3  = [x for x in image_list_3 if '.ipynb_checkpoints' not in x]"
      ],
      "metadata": {
        "id": "7kyw3-_pKDWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(image_list_3)"
      ],
      "metadata": {
        "id": "Grdr8NC1Kdrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "!pip install -q xlrd"
      ],
      "metadata": {
        "id": "7w6Kniclb0qW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_excel('/content/drive/My Drive/FYP/ECGpdf2data/dataset/12_lead_ECG/12_lead_ECG_DL_train.xlsx')\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "id": "dv89_JgjbPLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=df['ID'].values\n",
        "\n",
        "data_=data.tolist()"
      ],
      "metadata": {
        "id": "4J0PROvLjBO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(data_[0])"
      ],
      "metadata": {
        "id": "roS6QrWipbUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from contextlib import ExitStack\n",
        "results=[]\n",
        "for data in data_:\n",
        "  \n",
        "  # if(df['ID'] != int(image[0:-4])):\n",
        "  #   print(image[0:-4])\n",
        "  exist=0\n",
        "  for i in range(0,len(image_list)):\n",
        "    if str(data) == image_list[i]: exist=1\n",
        "  if(exist==0):\n",
        "    results.append(data)\n",
        "# results  = [x for x in data_ if x not in image_list]\n",
        "  # result = df[df['ID'] != int(image[0:-4])]\n",
        "  # print(result)\n",
        "  # label = result['Diagnosis'].values[0]  # 应该加在default loader里面\n",
        "  # mask_target.append(label)"
      ],
      "metadata": {
        "id": "6VHDLDxMI0Ud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(results)"
      ],
      "metadata": {
        "id": "t2qnWGG-njd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "id": "bKl73Zlwnk5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Jpg to png"
      ],
      "metadata": {
        "id": "7d3d-HOwJbIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "imagr_folder=('/content/drive/My Drive/FYP_Yixin_Cai/GAA_data/test/digitized_paper')\n",
        "\n"
      ],
      "metadata": {
        "id": "B8TgSkSJoQOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_list = os.listdir(imagr_folder)\n",
        "image_list = [x for x in image_list if '.DS_Store' not in x]\n",
        "image_list = [x for x in image_list if '.ipynb_checkpoints' not in x]"
      ],
      "metadata": {
        "id": "TGgPeS3CJ0VB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for image in image_list:\n",
        "  im1 = Image. open(r'/content/drive/My Drive/FYP_Yixin_Cai/GAA_data/test/digitized_paper/'+image)\n",
        "  im1. save(r'/content/drive/My Drive/FYP_Yixin_Cai/GAA_data/test/digitized_paper2/'+image[0:-4]+'.png')\n",
        "  "
      ],
      "metadata": {
        "id": "9FewBxT7JtJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.mkdir('/content/drive/My Drive/FYP_Yixin_Cai/GAA_data/test/output2/binary_image_5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "id": "On0frHyp1pfh",
        "outputId": "5b413bc9-6eb3-4269-eb19-29a550471ac9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileExistsError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-772d5e2f4f5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/FYP_Yixin_Cai/GAA_data/test/output2/binary_image_5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: '/content/drive/My Drive/FYP_Yixin_Cai/GAA_data/test/output2/binary_image_5'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from os import path\n",
        "\n",
        "if path.exists('/content/drive/My Drive/FYP_Yixin_Cai/GAA_data/test/output2/binary_image_5') == False:\n",
        "  os.mkdir('/content/drive/My Drive/FYP_Yixin_Cai/GAA_data/test/output2/binary_image_5')"
      ],
      "metadata": {
        "id": "gZnQCd7e3W_C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}